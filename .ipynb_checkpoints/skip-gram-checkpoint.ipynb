{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T15:35:25.428144Z",
     "start_time": "2020-07-24T15:35:24.409380Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T15:35:25.435010Z",
     "start_time": "2020-07-24T15:35:25.430548Z"
    }
   },
   "outputs": [],
   "source": [
    "negative_sample_size = 100\n",
    "window_size = 3\n",
    "embedding_size = 100\n",
    "max_vocab_size = 30000\n",
    "\n",
    "n_epochs = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.05\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T15:35:29.185585Z",
     "start_time": "2020-07-24T15:35:25.437055Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = '/Users/ed/Downloads/text8/text8.train.txt'\n",
    "\n",
    "with open(file_path) as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text.lower().split()\n",
    "\n",
    "vocab_dict = dict(Counter(text).most_common(max_vocab_size - 1))\n",
    "vocab_dict['UNK'] = len(text) - np.sum(list(vocab_dict.values()))\n",
    "\n",
    "word2idx = dict(zip(vocab_dict.keys(), range(len(vocab_dict))))\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T15:35:29.197455Z",
     "start_time": "2020-07-24T15:35:29.188909Z"
    }
   },
   "outputs": [],
   "source": [
    "word_counts = np.array(list(vocab_dict.values()))\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "word_freqs = word_freqs ** (3 / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T15:35:29.209770Z",
     "start_time": "2020-07-24T15:35:29.200588Z"
    }
   },
   "outputs": [],
   "source": [
    "class WordEmbeddingDataSet(Dataset):\n",
    "    def __init__(self, text, word2idx, idx2word, word_freqs, word_counts, negative_sample_size=5, window_size=2):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.text_encode = [word2idx.get(word, word2idx['UNK']) for word in text]\n",
    "        self.text_encode = torch.LongTensor(self.text_encode)\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.negative_sample_size = negative_sample_size\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.word_freqs = word_freqs\n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_encode)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        center_words = self.text_encode[idx]\n",
    "        pos_indices = list(range(idx - self.window_size, idx)) + list(range(idx + 1, idx + 1 + self.window_size))\n",
    "        pos_indices = [i % len(self.text_encode) for i in pos_indices]\n",
    "        pos_words = self.text_encode[pos_indices]\n",
    "        neg_words = torch.multinomial(self.word_counts, self.negative_sample_size * pos_words.shape[0],\n",
    "                                      replacement=True)\n",
    "        # 注意这三个都是 longTensor 类型的\n",
    "        return center_words, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T15:35:31.826822Z",
     "start_time": "2020-07-24T15:35:29.211820Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataSet(text, word2idx, idx2word, word_freqs, word_counts, negative_sample_size, window_size)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T15:35:31.835093Z",
     "start_time": "2020-07-24T15:35:31.828551Z"
    }
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # 每个单词有两个 embedding\n",
    "        self.u_embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.v_embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "\n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        center_embedding = self.u_embedding(input_labels)\n",
    "        pos_embedding = self.v_embedding(pos_labels)\n",
    "        neg_embedding = self.v_embedding(neg_labels)\n",
    "\n",
    "        center_embedding = center_embedding.unsqueeze(2)\n",
    "        pos_dot = torch.bmm(pos_embedding, center_embedding)\n",
    "        pos_dot = pos_dot.squeeze(2)\n",
    "\n",
    "        neg_dot = torch.bmm(neg_embedding, -center_embedding)\n",
    "        neg_dot = neg_dot.squeeze(2)\n",
    "\n",
    "        log_pos = F.logsigmoid(pos_dot).sum(axis=1)\n",
    "        log_neg = F.logsigmoid(neg_dot).sum(axis=1)\n",
    "        loss = (log_pos + log_neg).mean()\n",
    "\n",
    "        return -loss\n",
    "\n",
    "    def input_embedding(self):\n",
    "        return self.u_embedding.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T15:35:31.879789Z",
     "start_time": "2020-07-24T15:35:31.836951Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SkipGram(vocab_size=max_vocab_size, embedding_size=embedding_size)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-24T14:05:15.743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1 Step: 500 Loss: 123.228515625 time_used: 162.28567790985107\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 500 Loss: 123.228515625 time_used: 162.28567790985107\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 1000 Loss: 94.71598052978516 time_used: 172.76516699790955\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 1000 Loss: 94.71598052978516 time_used: 172.76516699790955\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 1500 Loss: 81.81523132324219 time_used: 213.56765604019165\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 1500 Loss: 81.81523132324219 time_used: 213.56765604019165\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 2000 Loss: 89.66539001464844 time_used: 198.16432213783264\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 2000 Loss: 89.66539001464844 time_used: 198.16432213783264\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 2500 Loss: 68.77961730957031 time_used: 193.19138479232788\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 2500 Loss: 68.77961730957031 time_used: 193.19138479232788\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 3000 Loss: 79.43632507324219 time_used: 187.9019479751587\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 3000 Loss: 79.43632507324219 time_used: 187.9019479751587\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 3500 Loss: 76.32978820800781 time_used: 174.97026109695435\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 3500 Loss: 76.32978820800781 time_used: 174.97026109695435\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 4000 Loss: 93.98535919189453 time_used: 167.44504284858704\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 4000 Loss: 93.98535919189453 time_used: 167.44504284858704\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 4500 Loss: 83.65129852294922 time_used: 167.27966713905334\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 4500 Loss: 83.65129852294922 time_used: 167.27966713905334\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 5000 Loss: 70.00355529785156 time_used: 174.23818397521973\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 5000 Loss: 70.00355529785156 time_used: 174.23818397521973\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 5500 Loss: 63.75766372680664 time_used: 203.77569198608398\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 5500 Loss: 63.75766372680664 time_used: 203.77569198608398\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 6000 Loss: 73.43788146972656 time_used: 188.1856780052185\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 6000 Loss: 73.43788146972656 time_used: 188.1856780052185\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 6500 Loss: 71.58832550048828 time_used: 203.15359091758728\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 6500 Loss: 71.58832550048828 time_used: 203.15359091758728\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 7000 Loss: 80.50999450683594 time_used: 196.82081818580627\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 7000 Loss: 80.50999450683594 time_used: 196.82081818580627\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 7500 Loss: 78.89712524414062 time_used: 187.40820002555847\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 7500 Loss: 78.89712524414062 time_used: 187.40820002555847\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 8000 Loss: 88.3829116821289 time_used: 168.68424701690674\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 8000 Loss: 88.3829116821289 time_used: 168.68424701690674\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 8500 Loss: 68.02147674560547 time_used: 168.84719800949097\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 8500 Loss: 68.02147674560547 time_used: 168.84719800949097\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 9000 Loss: 77.12126159667969 time_used: 166.67048501968384\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 9000 Loss: 77.12126159667969 time_used: 166.67048501968384\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 9500 Loss: 80.34209442138672 time_used: 165.19751811027527\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 9500 Loss: 80.34209442138672 time_used: 165.19751811027527\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 10000 Loss: 82.49769592285156 time_used: 166.57506012916565\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 10000 Loss: 82.49769592285156 time_used: 166.57506012916565\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 10500 Loss: 95.11157989501953 time_used: 169.4424741268158\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 10500 Loss: 95.11157989501953 time_used: 169.4424741268158\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 11000 Loss: 84.51838684082031 time_used: 164.90177011489868\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 11000 Loss: 84.51838684082031 time_used: 164.90177011489868\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 11500 Loss: 74.04930114746094 time_used: 162.38203501701355\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 11500 Loss: 74.04930114746094 time_used: 162.38203501701355\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 12000 Loss: 77.82303619384766 time_used: 162.80354499816895\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 12000 Loss: 77.82303619384766 time_used: 162.80354499816895\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 12500 Loss: 87.43818664550781 time_used: 162.38419699668884\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 12500 Loss: 87.43818664550781 time_used: 162.38419699668884\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 13000 Loss: 70.9836654663086 time_used: 162.69675397872925\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 13000 Loss: 70.9836654663086 time_used: 162.69675397872925\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 13500 Loss: 72.0966796875 time_used: 162.4540479183197\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 13500 Loss: 72.0966796875 time_used: 162.4540479183197\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 14000 Loss: 80.369140625 time_used: 163.16217803955078\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 14000 Loss: 80.369140625 time_used: 163.16217803955078\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 14500 Loss: 70.84269714355469 time_used: 163.24330711364746\n",
      "Model saved successfully!\n",
      "Epoch: 0/1 Step: 14500 Loss: 70.84269714355469 time_used: 163.24330711364746\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "while epoch < n_epochs:\n",
    "    time_start = time.time()\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader, start=1):\n",
    "        input_labels = input_labels.to(device)\n",
    "        pos_labels = pos_labels.to(device)\n",
    "        neg_labels = neg_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(\"Epoch: {}/{} Step: {} Loss: {} time_used: {}\".format(epoch, n_epochs, i, loss.item(),\n",
    "                                                                        time.time() - time_start))\n",
    "            time_start = time.time()\n",
    "            for file in os.listdir():\n",
    "                if file.endswith(\"pth\"):\n",
    "                    os.remove(file)\n",
    "            torch.save(model.state_dict(), \"embedding-{}.pth\".format(i))\n",
    "            print(\"Model saved successfully!\")\n",
    "    epoch += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T15:37:45.955769Z",
     "start_time": "2020-07-24T15:37:45.877697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two ['two', 'pandas', 'retailers', 'horrified', 'hellenes', 'embraced', 'sepulchre', 'investigated', 'built', 'stones']\n",
      "america ['america', 'iridium', 'uninterested', 'organisational', 'humid', 'foam', 'achievable', 'exhibition', 'have', 'bald']\n",
      "computer ['computer', 'cant', 'pistons', 'asians', 'cosimo', 'file', 'sharpness', 'beans', 'addicts', 'fractals']\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"embedding-14500.pth\"))\n",
    "embedding_weights = model.input_embedding()\n",
    "\n",
    "def find_nearest(word):\n",
    "    index = word2idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    cos_dis = cosine_distances(embedding.reshape(1, -1), embedding_weights).squeeze()\n",
    "    return [idx2word[i] for i in cos_dis.argsort()[:10]]\n",
    "\n",
    "\n",
    "for word in [\"two\", \"america\", \"computer\"]:\n",
    "    print(word, find_nearest(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. [PyTorch实现Word2Vec - mathor](https://wmathor.com/index.php/archives/1435/)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
