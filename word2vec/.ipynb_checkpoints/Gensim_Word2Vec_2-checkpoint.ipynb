{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[toc]\n",
    "\n",
    "# Gensim 的 Word2Vec 实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本的创建和训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:17:11.734446Z",
     "start_time": "2020-07-26T14:17:11.717803Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "\n",
    "raw_sentences = [\"the quick brown fox jumps over the lazy dogs\",\"yoyoyo you go home now to sleep\"]\n",
    "sentences = [s.split() for s in raw_sentences]\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, min_count=1) # 会直接训练并返回训练后的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用到的参数\n",
    "\n",
    "- size : int, optional\n",
    "    - Dimensionality of the word vectors\n",
    "- window : int, optional\n",
    "    - Maximum distance between the current and predicted word within a sentence.\n",
    "- min_count : int, optional\n",
    "    - Ignores all words with total frequency lower than this.\n",
    "- sg : {0, 1}, optional\n",
    "    - Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "- hs : {0, 1}, optional\n",
    "    - If 1, hierarchical softmax will be used for model training.\n",
    "    - If 0, and `negative` is non-zero, negative sampling will be used.\n",
    "- negative : int, optional\n",
    "    If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
    "    should be drawn (usually between 5-20).\n",
    "    If set to 0, no negative sampling is used.\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 train 来训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码在构建模型的时候会自动训练。还有一种方式实际上是使用 `model.build_vocab()` 后再使用 `model.train()` 来训练！这种方式更加灵活，可以支持增量训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:17:15.884371Z",
     "start_time": "2020-07-26T14:17:15.866281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 80)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(min_count=1)\n",
    "model.build_vocab(sentences) # 这个用来建立词汇表，但是不训练\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，上面的方法在底层也是调用了 `.build_vocab()` 和 `model.train()` 来实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim 中的 Word2Vec 类继承自 `BaseWordEmbeddingsModel` 而在 `BaseWordEmbeddingsModel` 类中有下面的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "if sentences is not None or corpus_file is not None:\n",
    "    self._check_input_data_sanity(data_iterable=sentences, corpus_file=corpus_file)\n",
    "    if corpus_file is not None and not isinstance(corpus_file, string_types):\n",
    "        raise TypeError(\"You must pass string as the corpus_file argument.\")\n",
    "    elif isinstance(sentences, GeneratorType):\n",
    "        raise TypeError(\"You can't pass a generator as the sentences argument. Try a sequence.\")\n",
    "\n",
    "    self.build_vocab(sentences=sentences, corpus_file=corpus_file, trim_rule=trim_rule)\n",
    "    self.train(\n",
    "        sentences=sentences, corpus_file=corpus_file, total_examples=self.corpus_count,\n",
    "        total_words=self.corpus_total_words, epochs=self.epochs, start_alpha=self.alpha,\n",
    "        end_alpha=self.min_alpha, compute_loss=compute_loss)\n",
    "else:\n",
    "    if trim_rule is not None:\n",
    "        logger.warning(\n",
    "            \"The rule, if given, is only used to prune vocabulary during build_vocab() \"\n",
    "            \"and is not stored as part of the model. Model initialized without sentences. \"\n",
    "            \"trim_rule provided, if any, will be ignored.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:51:09.583965Z",
     "start_time": "2020-07-26T14:51:09.575445Z"
    }
   },
   "source": [
    "可以看到，如果在模型构建的时候传入 sentences 参数或 corpus_file 参数，那么就会在进行一些检查后调用 build_vocab 和 train。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的一些属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:19:59.085516Z",
     "start_time": "2020-07-26T14:19:59.081326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count # 所有的句子数\n",
    "model.corpus_total_words # 所有的单词数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从多个文件中输入语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:10:24.564326Z",
     "start_time": "2020-07-26T14:10:24.536994Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2f9f02b149ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMySentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/some/directory'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# a memory-friendly iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             self.train(\n\u001b[1;32m    747\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m         \"\"\"\n\u001b[1;32m    921\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 922\u001b[0;31m             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m   1401\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         logger.info(\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2f9f02b149ac>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "\n",
    "sentences = MySentences('/some/directory') # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型保存与读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:10:37.206274Z",
     "start_time": "2020-07-26T14:10:37.198779Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('test.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:23:21.316994Z",
     "start_time": "2020-07-26T14:23:21.310237Z"
    }
   },
   "outputs": [],
   "source": [
    "restored_model = gensim.models.word2vec.Word2Vec.load(\"test.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存成二进制文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:32:00.187982Z",
     "start_time": "2020-07-25T14:32:00.182606Z"
    }
   },
   "outputs": [],
   "source": [
    "binary_save_path = \"test.model.bin\"\n",
    "model.wv.save_word2vec_format(binary_save_path, binary=True)\n",
    "\n",
    "restored_binary_model = gensim.models.KeyedVectors.load_word2vec_format(binary_save_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:32:33.830975Z",
     "start_time": "2020-07-25T14:32:33.828020Z"
    }
   },
   "source": [
    "## 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:41:14.329199Z",
     "start_time": "2020-07-25T14:41:14.323106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': <gensim.models.keyedvectors.Vocab at 0x11641a208>,\n",
       " 'quick': <gensim.models.keyedvectors.Vocab at 0x11641a240>,\n",
       " 'brown': <gensim.models.keyedvectors.Vocab at 0x11641a2b0>,\n",
       " 'fox': <gensim.models.keyedvectors.Vocab at 0x11641a2e8>,\n",
       " 'jumps': <gensim.models.keyedvectors.Vocab at 0x11641a320>,\n",
       " 'over': <gensim.models.keyedvectors.Vocab at 0x11641a358>,\n",
       " 'lazy': <gensim.models.keyedvectors.Vocab at 0x11641a390>,\n",
       " 'dogs': <gensim.models.keyedvectors.Vocab at 0x11641a3c8>,\n",
       " 'yoyoyo': <gensim.models.keyedvectors.Vocab at 0x11641a400>,\n",
       " 'you': <gensim.models.keyedvectors.Vocab at 0x11641a438>,\n",
       " 'go': <gensim.models.keyedvectors.Vocab at 0x11641a470>,\n",
       " 'home': <gensim.models.keyedvectors.Vocab at 0x11641a4a8>,\n",
       " 'now': <gensim.models.keyedvectors.Vocab at 0x11641a4e0>,\n",
       " 'to': <gensim.models.keyedvectors.Vocab at 0x11641a518>,\n",
       " 'sleep': <gensim.models.keyedvectors.Vocab at 0x11641a550>}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab # 词典中的所有词汇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:33:13.395243Z",
     "start_time": "2020-07-25T14:33:13.392720Z"
    }
   },
   "source": [
    "### 获取词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:33:02.620548Z",
     "start_time": "2020-07-25T14:33:02.613908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.46173847e-04, -1.18479526e-04,  3.21807084e-03,  4.99845808e-03,\n",
       "        4.77842428e-03, -3.48846824e-03, -4.29400010e-03,  1.23152218e-04,\n",
       "       -2.46726978e-03,  3.05278203e-03,  3.72302555e-03,  4.91536362e-03,\n",
       "        2.10555480e-03,  3.03062989e-04, -1.67109061e-03, -3.17640114e-03,\n",
       "       -3.49465734e-03, -2.93565402e-03, -2.12275982e-03,  4.75081988e-03,\n",
       "       -4.52112732e-03,  1.27372611e-03, -1.47000610e-04,  4.88099689e-03,\n",
       "       -1.28948118e-03,  2.26798770e-03,  1.00740278e-03, -4.67300368e-03,\n",
       "        3.89396190e-03,  3.68328160e-03, -4.92732320e-03,  4.78392380e-04,\n",
       "       -3.48263746e-03,  1.35074486e-03,  3.89290787e-03,  2.35763728e-03,\n",
       "        1.96083076e-03, -4.07436350e-03,  1.82393834e-03, -2.06615823e-05,\n",
       "       -2.42632627e-03, -8.64281319e-04, -2.38293223e-03,  1.37142965e-03,\n",
       "       -7.16688577e-04,  3.39390291e-03, -2.94442616e-05,  3.78515688e-03,\n",
       "       -5.90594078e-04,  2.93036830e-03, -7.90181337e-04, -9.00543644e-04,\n",
       "       -2.27597402e-03,  2.39972002e-03,  1.06817705e-03,  1.61752547e-03,\n",
       "        3.07261851e-03, -3.37813841e-03, -1.46935519e-03, -2.85821571e-03,\n",
       "       -4.04541008e-03,  3.50955571e-03, -6.74419396e-04, -1.90440522e-04,\n",
       "       -1.24030525e-03,  3.70091555e-04, -3.20983655e-03,  4.95446473e-03,\n",
       "       -1.83216028e-03, -1.24702125e-03, -4.52870363e-03,  1.73926691e-03,\n",
       "        2.90717743e-03, -3.41100595e-03,  2.34580901e-03, -1.07286079e-03,\n",
       "        6.22604450e-04,  4.40633809e-03,  1.64472766e-03, -1.76863628e-03,\n",
       "        4.27685445e-03, -3.35264392e-03,  3.40866647e-03, -2.55599688e-03,\n",
       "       -1.31452957e-03,  2.13949825e-03, -4.67571896e-03, -4.25773999e-03,\n",
       "       -4.09237435e-03, -4.95744962e-03,  7.00462551e-04, -3.78034473e-03,\n",
       "        8.78224182e-06, -2.09247763e-03,  1.39067147e-03,  2.27848324e-03,\n",
       "        2.68212100e-03, -1.64957333e-03,  2.31947168e-03, -1.96570926e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['you']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取和某个单词最相似的前 topn 个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:36:37.252050Z",
     "start_time": "2020-07-25T14:36:37.246287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('over', 0.23280760645866394),\n",
       " ('sleep', 0.09017609059810638),\n",
       " ('home', 0.08646240830421448)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar([\"you\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:39:42.131297Z",
     "start_time": "2020-07-25T14:39:42.128045Z"
    }
   },
   "source": [
    "### 做 xx 之于 xx = xx 之于 xx 的推理题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:39:48.871777Z",
     "start_time": "2020-07-25T14:39:48.857252Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'woman' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-9caa1fcafbfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'woman'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'king'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'man'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'woman' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:38:19.383235Z",
     "start_time": "2020-07-25T14:38:19.381293Z"
    }
   },
   "source": [
    "### 计算两个单词之间的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:37:27.254817Z",
     "start_time": "2020-07-25T14:37:27.248411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.059096213"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"fox\", \"dogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:42:27.153322Z",
     "start_time": "2020-07-25T14:42:27.147823Z"
    }
   },
   "source": [
    "# References\n",
    "1. [基于 Gensim 的 Word2Vec 实践 - 知乎](https://zhuanlan.zhihu.com/p/24961011)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tensorflow2': conda)",
   "language": "python",
   "name": "python361064bittensorflow2conda916f6dc8789a43e39b82205c8a731f83"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
